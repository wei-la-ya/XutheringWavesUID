import copy
import json
import base64
import asyncio
from typing import Dict, List, Tuple, Union, Optional
from pathlib import Path
from datetime import datetime

import msgspec
import aiofiles

from gsuid_core.logger import logger
from gsuid_core.models import Event

from .model import WWUIDGacha
from ..version import XutheringWavesUID_version
from ..utils.api.api import WAVES_GAME_ID
from ..utils.api.model import GachaLog
from ..utils.waves_api import waves_api
from ..utils.database.models import WavesUser
from ..wutheringwaves_config import PREFIX
from .model_for_waves_plugin import WavesPluginGacha
from ..utils.resource.RESOURCE_PATH import PLAYER_PATH

gacha_type_meta_data = {
    "è§’è‰²ç²¾å‡†è°ƒè°": "1",
    "æ­¦å™¨ç²¾å‡†è°ƒè°": "2",
    "è§’è‰²è°ƒè°ï¼ˆå¸¸é©»æ± ï¼‰": "3",
    "æ­¦å™¨è°ƒè°ï¼ˆå¸¸é©»æ± ï¼‰": "4",
    "æ–°æ‰‹è°ƒè°": "5",
    "æ–°æ‰‹è‡ªé€‰å”¤å–": "6",
    "æ–°æ‰‹è‡ªé€‰å”¤å–ï¼ˆæ„Ÿæ©å®šå‘å”¤å–ï¼‰": "7",
    "è§’è‰²æ–°æ—…å”¤å–": "8",
    "æ­¦å™¨æ–°æ—…å”¤å–": "9",
}

gacha_type_meta_data_reverse = {v: k for k, v in gacha_type_meta_data.items()}

gachalogs_history_meta = {
    "è§’è‰²ç²¾å‡†è°ƒè°": [],
    "æ­¦å™¨ç²¾å‡†è°ƒè°": [],
    "è§’è‰²è°ƒè°ï¼ˆå¸¸é©»æ± ï¼‰": [],
    "æ­¦å™¨è°ƒè°ï¼ˆå¸¸é©»æ± ï¼‰": [],
    "æ–°æ‰‹è°ƒè°": [],
    "æ–°æ‰‹è‡ªé€‰å”¤å–": [],
    "æ–°æ‰‹è‡ªé€‰å”¤å–ï¼ˆæ„Ÿæ©å®šå‘å”¤å–ï¼‰": [],
    "è§’è‰²æ–°æ—…å”¤å–": [],
    "æ­¦å™¨æ–°æ—…å”¤å–": [],
}

ERROR_MSG_INVALID_LINK = "å½“å‰æŠ½å¡é“¾æ¥å·²ç»å¤±æ•ˆï¼Œè¯·é‡æ–°å¯¼å…¥æŠ½å¡é“¾æ¥"


# æ‰¾åˆ°ä¸¤ä¸ªæ•°ç»„ä¸­æœ€é•¿å…¬å…±å­ä¸²çš„ä¸‹æ ‡
def find_longest_common_subarray_indices(
    a: List[GachaLog], b: List[GachaLog]
) -> Optional[Tuple[Tuple[int, int], Tuple[int, int]]]:
    n, m = len(a), len(b)
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    length = 0
    a_end = b_end = 0

    for i in range(n - 1, -1, -1):
        for j in range(m - 1, -1, -1):
            if a[i] == b[j]:
                dp[i][j] = dp[i + 1][j + 1] + 1
                if dp[i][j] > length:
                    length = dp[i][j]
                    a_end = i + length - 1
                    b_end = j + length - 1
            else:
                dp[i][j] = 0

    if length == 0:
        return None

    return (a_end - length + 1, a_end), (b_end - length + 1, b_end)


# æ ¹æ®æœ€é•¿å…¬å…±å­ä¸²é€’å½’åˆå¹¶ä¸¤ä¸ªGachaLogåˆ—è¡¨ï¼Œä¸å»é‡ï¼ŒæŒ‰timeæ’åº
def merge_gacha_logs_by_common_subarray(a: List[GachaLog], b: List[GachaLog]) -> List[GachaLog]:
    common_indices = find_longest_common_subarray_indices(a, b)
    if not common_indices:
        return sorted(
            a + b,
            key=lambda log: datetime.strptime(log.time, "%Y-%m-%d %H:%M:%S"),
            reverse=True,
        )

    (a_start, a_end), (b_start, b_end) = common_indices

    prefix = merge_gacha_logs_by_common_subarray(a[:a_start], b[:b_start])
    common_subarray = a[a_start : a_end + 1]
    suffix = merge_gacha_logs_by_common_subarray(a[a_end + 1 :], b[b_end + 1 :])

    return prefix + common_subarray + suffix


async def get_new_gachalog(
    uid: str, record_id: str, full_data: Dict[str, List[GachaLog]], is_force: bool
) -> tuple[Union[str, None], Dict[str, List[GachaLog]], Dict[str, int], Dict[str, List[GachaLog]]]:
    new = {}
    new_count = {}
    link_source_data: Dict[str, List[GachaLog]] = {}
    for gacha_name, card_pool_type in gacha_type_meta_data.items():
        res = await waves_api.get_gacha_log(card_pool_type, record_id, uid)
        if not res.success or not res.data:
            # æŠ½å¡è®°å½•è·å–å¤±è´¥
            if res.code == -1:  # type: ignore
                return ERROR_MSG_INVALID_LINK, None, None, {}  # type: ignore

        if res.data and isinstance(res.data, list):
            temp = res.data
        else:
            temp = []

        gacha_log = [GachaLog.model_validate(log) for log in temp]  # type: ignore
        for log in gacha_log:
            if log.cardPoolType != card_pool_type:
                log.cardPoolType = card_pool_type
        link_source_data[gacha_name] = list(gacha_log)
        common_indices = find_longest_common_subarray_indices(full_data[gacha_name], gacha_log)
        if not common_indices:
            _add = gacha_log
        else:
            (_, _), (b_start, b_end) = common_indices
            _add = gacha_log[:b_start]
        new[gacha_name] = _add + copy.deepcopy(full_data[gacha_name])
        new_count[gacha_name] = len(_add)
        await asyncio.sleep(1)

    return None, new, new_count, link_source_data


async def get_new_gachalog_for_file(
    full_data: Dict[str, List[GachaLog]],
    import_data: Dict[str, List[GachaLog]],
) -> tuple[Union[str, None], Dict[str, List[GachaLog]], Dict[str, int]]:
    new = {}
    new_count = {}

    if str(full_data) == str(import_data):
        for gacha_name, logs in full_data.items():
            new[gacha_name] = list(logs)
            new_count[gacha_name] = len(logs)
        return None, new, new_count

    for cardPoolType, item in import_data.items():
        item: List[GachaLog]
        if cardPoolType not in gacha_type_meta_data:
            continue
        gacha_name = cardPoolType
        gacha_log = [GachaLog(**log.dict()) for log in item]
        new_gacha_log = merge_gacha_logs_by_common_subarray(full_data[gacha_name], gacha_log)
        new[gacha_name] = new_gacha_log
        new_count[gacha_name] = len(new_gacha_log)
    return None, new, new_count


async def backup_gachalogs(uid: str, gachalogs_history: Dict, type: str):
    path = PLAYER_PATH / str(uid)
    if not path.exists():
        path.mkdir(parents=True, exist_ok=True)
    # å¤‡ä»½
    backup_path = path / f"{type}_gacha_logs_{datetime.now().strftime('%Y-%m-%d.%H%M%S')}.json"
    async with aiofiles.open(backup_path, "w", encoding="UTF-8") as file:
        await file.write(json.dumps(gachalogs_history, ensure_ascii=False))


async def save_link_source_gachalogs(uid: str, record_id: str, data: Dict[str, List[GachaLog]]):
    """ä¿å­˜é€šè¿‡é“¾æ¥è·å–çš„æŠ½å¡åŸå§‹æ•°æ®"""
    path = PLAYER_PATH / str(uid)
    if not path.exists():
        path.mkdir(parents=True, exist_ok=True)

    content = {
        "uid": uid,
        "record_id": record_id,
        "fetch_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "data": {gacha_name: [log.dict() for log in logs] for gacha_name, logs in data.items()},
    }

    async with aiofiles.open(path / "link_gacha_logs.json", "w", encoding="UTF-8") as file:
        await file.write(json.dumps(content, ensure_ascii=False, indent=2))


async def save_gachalogs(
    ev: Event,
    uid: str,
    record_id: str,
    is_force: bool = False,
    import_data: Optional[Dict[str, List[GachaLog]]] = None,
    force_overwrite: bool = False,
) -> str:
    path = PLAYER_PATH / str(uid)
    if not path.exists():
        path.mkdir(parents=True, exist_ok=True)

    # æŠ½å¡è®°å½•jsonè·¯å¾„
    gachalogs_path = path / "gacha_logs.json"

    temp_gachalogs_history = {}
    if gachalogs_path.exists():
        with Path.open(gachalogs_path, encoding="UTF-8") as f:
            gachalogs_history: Dict = json.load(f)

        # import æ—¶å¤‡ä»½
        if not record_id:
            await backup_gachalogs(uid, gachalogs_history, type="import")

        # update æ—¶å¤‡ä»½
        temp_gachalogs_history = copy.deepcopy(gachalogs_history)

        gachalogs_history = gachalogs_history["data"]
    else:
        gachalogs_history = copy.deepcopy(gachalogs_history_meta)

    temp = copy.deepcopy(gachalogs_history_meta)
    temp.update(gachalogs_history)
    gachalogs_history = temp

    is_need_backup = False
    for gacha_name, card_pool_type in gacha_type_meta_data.items():
        for log in range(len(gachalogs_history[gacha_name]) - 1, -1, -1):
            pool_type = gachalogs_history[gacha_name][log]["cardPoolType"]
            if pool_type == card_pool_type:
                continue
            if card_pool_type == "æ­¦å™¨ç²¾å‡†è°ƒè°" and pool_type == "è§’è‰²ç²¾å‡†è°ƒè°-2":
                del gachalogs_history[gacha_name][log]
            elif card_pool_type == "è§’è‰²è°ƒè°ï¼ˆå¸¸é©»æ± ï¼‰" and pool_type == "æ­¦å™¨ç²¾å‡†è°ƒè°":
                del gachalogs_history[gacha_name][log]
            elif card_pool_type == "æ­¦å™¨è°ƒè°ï¼ˆå¸¸é©»æ± ï¼‰" and pool_type == "å…¨é¢‘è°ƒè°":
                del gachalogs_history[gacha_name][log]
            else:
                gachalogs_history[gacha_name][log]["cardPoolType"] = card_pool_type

            is_need_backup = True

    if is_need_backup:
        await backup_gachalogs(uid, temp_gachalogs_history, type="update")

    for gacha_name in gacha_type_meta_data.keys():
        gachalogs_history[gacha_name] = [GachaLog(**log) for log in gachalogs_history[gacha_name]]

    link_source_data: Dict[str, List[GachaLog]] = {}
    if record_id:
        code, gachalogs_new, gachalogs_count_add, link_source_data = await get_new_gachalog(
            uid, record_id, gachalogs_history, is_force
        )
    elif not force_overwrite:
        code, gachalogs_new, gachalogs_count_add = await get_new_gachalog_for_file(
            gachalogs_history,
            import_data,  # type: ignore
        )
    else:
        code, gachalogs_new, gachalogs_count_add = await get_new_gachalog_for_file(
            import_data,
            import_data,  # type: ignore
        )

    if isinstance(code, str) or not gachalogs_new:
        return code or ERROR_MSG_INVALID_LINK

    if record_id:
        await save_record_id(ev.user_id, ev.bot_id, uid, record_id)
        if link_source_data:
            await save_link_source_gachalogs(uid, record_id, link_source_data)

    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now().strftime("%Y-%m-%d %H-%M-%S")

    # æ£€æŸ¥å¹¶ä¿®æ­£æ—¶é—´é™åº
    for gacha_name in gacha_type_meta_data.keys():
        logs = gachalogs_new.get(gacha_name, [])
        if len(logs) > 1:
            # ä»æœ«å°¾å€’ç€æ£€æŸ¥æ—¶é—´é¡ºåº
            for i in range(len(logs) - 1, 0, -1):
                time_current = datetime.strptime(logs[i].time, "%Y-%m-%d %H:%M:%S")
                time_prev = datetime.strptime(logs[i - 1].time, "%Y-%m-%d %H:%M:%S")

                # å¦‚æœç¬¬ i-1 ä¸ªçš„æ—¶é—´å°äºç¬¬ i ä¸ªï¼Œè¯´æ˜é¡ºåºä¸å¯¹ï¼Œèˆå¼ƒ i-1 åŠä¹‹å‰çš„æ‰€æœ‰è®°å½•
                if time_prev < time_current:
                    logger.warning(f"[{gacha_name}] å‘ç°æ—¶é—´é¡ºåºå¼‚å¸¸ï¼Œèˆå¼ƒç´¢å¼• {i - 1} åŠä¹‹å‰çš„ {i} æ¡è®°å½•")
                    gachalogs_new[gacha_name] = logs[i:]
                    break

    # åˆå§‹åŒ–æœ€åä¿å­˜çš„æ•°æ®
    result = {"uid": uid, "data_time": current_time}

    # ä¿å­˜æ•°é‡
    for gacha_name in gacha_type_meta_data.keys():
        result[gacha_name] = len(gachalogs_new.get(gacha_name, []))  # type: ignore

    result["data"] = {  # type: ignore
        gacha_name: [log.dict() for log in gachalogs_new.get(gacha_name, [])]
        for gacha_name in gacha_type_meta_data.keys()
    }

    vo = msgspec.to_builtins(result)
    async with aiofiles.open(gachalogs_path, "w", encoding="UTF-8") as file:
        await file.write(json.dumps(vo, ensure_ascii=False))

    # è®¡ç®—æ•°æ®
    all_add = sum(gachalogs_count_add.values())

    # å›å¤æ–‡å­—
    im = []
    if all_add == 0:
        im.append(f"ğŸŒ±UID{uid}æ²¡æœ‰æ–°å¢è°ƒè°æ•°æ®!")
    else:
        im.append(f"âœ…UID{uid}æ•°æ®æ›´æ–°æˆåŠŸï¼")
        for k, v in gachalogs_count_add.items():
            im.append(f"[{k}]æ–°å¢{v}ä¸ªæ•°æ®ï¼")
    im.append(f"å¯ä»¥ä½¿ç”¨ã€{PREFIX}æŠ½å¡è®°å½•ã€‘è·å–å…¨éƒ¨æŠ½å¡æ•°æ®")
    im = "\n".join(im)
    return im


async def save_record_id(user_id, bot_id, uid, record_id):
    user = await WavesUser.get_user_by_attr(user_id, bot_id, "uid", uid, game_id=WAVES_GAME_ID)
    if user:
        if user.record_id == record_id:
            return
        await WavesUser.update_data_by_data(
            select_data={
                "user_id": user_id,
                "bot_id": bot_id,
                "uid": uid,
                "game_id": WAVES_GAME_ID,
            },
            update_data={"record_id": record_id},
        )
    else:
        await WavesUser.insert_data(user_id, bot_id, record_id=record_id, uid=uid, game_id=WAVES_GAME_ID)


async def import_gachalogs(ev: Event, history_url: str, type: str, uid: str, force_overwrite=False) -> str:
    history_data: Dict = {}
    if type == "json":
        history_data = json.loads(history_url)
    else:
        data_bytes = base64.b64decode(history_url)
        try:
            history_data = json.loads(data_bytes.decode())
        except UnicodeDecodeError:
            history_data = json.loads(data_bytes.decode("gbk"))
        except json.decoder.JSONDecodeError:
            return "è¯·ä¼ å…¥æ­£ç¡®çš„JSONæ ¼å¼æ–‡ä»¶!"

    def turn_wwuid_gacha(data: Dict) -> Optional[WWUIDGacha]:
        if "info" in data and "export_app" in data["info"]:
            if "Waves-Plugin" == data["info"]["export_app"]:
                return WavesPluginGacha.model_validate(data).turn_wwuid_gacha()
            elif "XutheringWavesUID" == data["info"]["export_app"] or "WutheringWavesUID" == data["info"]["export_app"]:
                return WWUIDGacha.model_validate(data)
        return None

    wwuid_gacha = turn_wwuid_gacha(history_data)
    if not wwuid_gacha:
        err_res = [
            "ä½ å½“å‰å¯¼å…¥çš„æŠ½å¡è®°å½•æ–‡ä»¶ä¸æ”¯æŒ, ç›®å‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹æœ‰:",
            "1.WutheringWavesUID",
            "2.XutheringWavesUID",
            "3.Waves-Plugin",
        ]
        return "\n".join(err_res)

    if wwuid_gacha.info.uid != uid:
        return "ä½ å½“å‰å¯¼å…¥çš„æŠ½å¡è®°å½•æ–‡ä»¶çš„UIDä¸å½“å‰UIDä¸åŒ¹é…!"

    import_data = copy.deepcopy(gachalogs_history_meta)
    for item in wwuid_gacha.list:
        gacha_name = item.cardPoolType
        if gacha_name in gacha_type_meta_data:
            # æ­¤æ—¶cardPoolTypeæ˜¯åå­— -> å¦‚è§’è‰²ç²¾å‡†è°ƒè°
            item.cardPoolType = gacha_type_meta_data[gacha_name]
        else:
            # æ­¤æ—¶cardPoolTypeæ˜¯ç±»å‹ -> å¦‚ "1"
            gacha_name = gacha_type_meta_data_reverse.get(item.cardPoolType)
            if not gacha_name:
                continue
        import_data[gacha_name].append(GachaLog(**item.dict()))

    res = await save_gachalogs(ev, uid, "", import_data=import_data, force_overwrite=force_overwrite)
    return res


async def export_gachalogs(uid: str) -> dict:
    path = PLAYER_PATH / uid
    if not path.exists():
        path.mkdir(parents=True, exist_ok=True)

    # è·å–å½“å‰æ—¶é—´
    now = datetime.now()
    current_time = now.strftime("%Y-%m-%d %H:%M:%S")

    # æŠ½å¡è®°å½•jsonè·¯å¾„
    gachalogs_path = path / "gacha_logs.json"
    if gachalogs_path.exists():
        async with aiofiles.open(gachalogs_path, "r", encoding="UTF-8") as f:
            raw_data = json.loads(await f.read())

        result = {
            "info": {
                "export_time": current_time,
                "export_app": "XutheringWavesUID",
                "export_app_version": XutheringWavesUID_version,
                "export_timestamp": round(now.timestamp()),
                "version": "v2.0",
                "uid": uid,
            },
            "list": [],
        }
        gachalogs_history = raw_data["data"]
        for name, gachalogs in gachalogs_history.items():
            result["list"].extend(gachalogs)

        async with aiofiles.open(path / f"export_{uid}.json", "w", encoding="UTF-8") as file:
            await file.write(json.dumps(result, ensure_ascii=False, indent=4))

        logger.success("[å¯¼å‡ºæŠ½å¡è®°å½•] å¯¼å‡ºæˆåŠŸ!")
        im = {
            "retcode": "ok",
            "data": "å¯¼å‡ºæˆåŠŸ!",
            "name": f"export_{uid}.json",
            "url": str((path / f"export_{uid}.json").absolute()),
        }
    else:
        logger.error("[å¯¼å‡ºæŠ½å¡è®°å½•] æ²¡æœ‰æ‰¾åˆ°æŠ½å¡è®°å½•!")
        im = {
            "retcode": "error",
            "data": "ä½ è¿˜æ²¡æœ‰æŠ½å¡è®°å½•å¯ä»¥å¯¼å‡º!",
            "name": "",
            "url": "",
        }

    return im
